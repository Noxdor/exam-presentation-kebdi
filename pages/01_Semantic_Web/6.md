---
layout: center
---

## Natural Language Processing


---
layout: center
class: "text-justify"
---

> Given the following text, do a preprocessing of it using 
> either the library **Spacy** (chosen) or **NLTK**
> (**Tokenisation**, **Stop-word Removal**, **Lemmatisation**)

“Every year we go to Florida. We like to go to the beach.
My favorite beach is called Emerson Beach. It is very long, with soft sand and palm trees.
It is very beautiful. I like to make sandcastles and watch the sailboats go by.
Sometimes there are dolphins and whales in the water!
Every morning we look for shells in the sand.
I found fifteen big shells last year.
I put them in a special place in my room. This year I want to learn to surf.
It is hard to surf, but so much fun! My sister is a good surfer.
She says that she can teach me. I hope I can do it!”

---
class: "text-justify"
---

The main python code loads a language model - here a model of the English language -
and parses the provided text with the loaded model.
The code then calls two functions: `filter_out_stop_words` and `print_lemmitisation`.
These functions are - together with the provided text - part of a second module called
`util`.

```python
import spacy
from util import text, print_lemmatisation, filter_out_stop_words


if __name__ == "__main__":
    nlp = spacy.load("en_core_web_sm")

    # Tokenisation
    doc = nlp(text)

    # Stop Word removal
    filter_out_stop_words(doc, nlp)

    print("\n")

    # Lemmatisation
    print_lemmatisation(doc)
```

---
class: "text-justify"
---

The module `util` contains the text and two functions. The first function is
`filter_out_stop_words`. It does what it says, it iterates over the by `Spacy`
created document and drops the words that are stop-words. With the rest of
the words, it creates a new doc and prints its content.

```python

import sys
from spacy.tokens import Doc
from spacy.language import Language

text: str = "..."


def filter_out_stop_words(doc: Doc, nlp: Language):
    words: list[str] = []

    for token in doc:
        if not token.is_stop:
            words.append(token.text)

    new_doc = Doc(nlp.vocab, words=words)
    print("Text with stop words removed is:\n")
    print(new_doc)
```

---
---

The result of the function call looks like this:


```
Text with stop words removed is:

year Florida . like beach . favorite beach called Emerson Beach . long , soft sand palm trees . beautiful . 
like sandcastles watch sailboats . dolphins whales water ! morning look shells sand . found big shells year . 
special place room . year want learn surf . hard surf , fun ! sister good surfer . says teach . hope ! 
```

---
class: "text-justify"
---

The second function contained in the module `util` is `print_lemmatisation`. It iterates over every token
in the doc and prints out both unprocessed form in which it appears in the text, as well as the lemma of
the word - that is, the stem or base form of the word. It prints the result in a table with 4 columns where
each cell contains a pair of `(text lemma)`.

```python
def print_lemmatisation(doc: Doc):
    i = 0
    sys.stdout.write("The Lemmatisation of the document is:\n\n")
    sys.stdout.write("-" * (2 * 12 + 4) * 4)
    sys.stdout.write("\n|")
    for token in doc:
        if i < 3:
            sys.stdout.write(f"{token.text: >12} {token.lemma_: >12} | ")
            i += 1
        else:
            sys.stdout.write(f"{token.text: >12} {token.lemma_: >12} |\n")
            sys.stdout.write("-" * (2 * 12 + 4) * 4)
            sys.stdout.write("\n|")
            i = 0
    sys.stdout.write("\n")
    sys.stdout.write("-" * (2 * 12 + 4) * i)
    print("")
```

---
---

The result of the function call looks like this:

```
The Lemmatisation of the document is:

----------------------------------------------------------------------------------------------------------------
|       Every        every |         year         year |           we           we |           go           go |
----------------------------------------------------------------------------------------------------------------
|          to           to |      Florida      Florida |            .            . |           We           we |
----------------------------------------------------------------------------------------------------------------
|        like         like |           to           to |           go           go |           to           to |
----------------------------------------------------------------------------------------------------------------
|         the          the |        beach        beach |            .            . |           My           my |
----------------------------------------------------------------------------------------------------------------
|    favorite     favorite |        beach        beach |           is           be |       called         call |
----------------------------------------------------------------------------------------------------------------
|     Emerson      Emerson |        Beach        Beach |            .            . |           It           it |
----------------------------------------------------------------------------------------------------------------
|          is           be |         very         very |         long         long |            ,            , |
----------------------------------------------------------------------------------------------------------------
|        with         with |         soft         soft |         sand         sand |          and          and |
----------------------------------------------------------------------------------------------------------------
|        palm         palm |        trees         tree |            .            . |           It           it |
----------------------------------------------------------------------------------------------------------------
|          is           be |         very         very |    beautiful    beautiful |            .            . |
----------------------------------------------------------------------------------------------------------------
...
```
